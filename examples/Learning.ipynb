{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learning ML model on non-aggregated data"
      ],
      "metadata": {
        "id": "Eult4u-UWW-D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzIbbdhdia_s"
      },
      "source": [
        "Welcome to this tutorial!\n",
        "\n",
        "We all know that data is required to train the model, and the quality of this data greatly affects the final result. Training a model on non-aggregated data usually leads to a loss in model quality, so the Toloka team has developed special layers for sublimating such data. In this notebook, we will solve this problem using the LSTM model and integrate the CoNAL and crowdlayer layers from Crowd-Kit library!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries importing"
      ],
      "metadata": {
        "id": "chiZAWvU_bGX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgWLac4VjpTv"
      },
      "source": [
        "First of all, let's install and import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neS64tr75OZs"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%pip install crowd-kit\n",
        "%pip install sentence_transformers\n",
        "from crowdkit.learning import CoNAL\n",
        "from crowdkit.learning import CrowdLayer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting with cuda"
      ],
      "metadata": {
        "id": "9fIbGnjO_8JJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T8-rLTjj-9F"
      },
      "source": [
        "It's important to boost our calculous speed so use the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VXUKMUa8tZz",
        "outputId": "f87079ed-7afd-4faa-a332-2439f14b9248"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "kkg5WEo9_3X5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP59rmCAkMET"
      },
      "source": [
        "We are going to use non-aggregated data. So, read the *train*, *val* and *test* data from csv-files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8H63lBr5lJe"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"fixed_train_crowd_alpha047 (1).csv\")\n",
        "val_data = pd.read_csv(\"fixed_val_clean_alpha047.csv\")\n",
        "test_data = pd.read_csv(\"fixed_test.csv\")\n",
        "pd.options.mode.chained_assignment = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpIa7B22kkrY"
      },
      "source": [
        "Preparing our data: choose the necessary columns and change labels from '*pos*'-'*neg*' to '*1*'-'0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6plSCtN5lMz"
      },
      "outputs": [],
      "source": [
        "train_data = train_data[['INPUT:text','OUTPUT:result','ASSIGNMENT:worker_id']]\n",
        "train_data['OUTPUT:result'] = train_data['OUTPUT:result'].replace('pos',int(1))\n",
        "train_data['OUTPUT:result'] = train_data['OUTPUT:result'].replace('neg',int(0))\n",
        "\n",
        "val_data = val_data[[\"text\", \"label\"]]\n",
        "val_data['label'] = val_data['label'].replace('pos',int(1))\n",
        "val_data['label'] = val_data['label'].replace('neg',int(0))\n",
        "\n",
        "test_data = test_data[['text', 'label']]\n",
        "test_data['label'] = test_data['label'].replace('pos',int(1))\n",
        "test_data['label'] = test_data['label'].replace('neg',int(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OKEjdb3l1KB"
      },
      "source": [
        "Since the ***id_worker*** field has the *string* type, we need to convert it to the *int* type. To do this, we will assign a unique values to all unique numbers in the field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25U3pe_48XRs"
      },
      "outputs": [],
      "source": [
        "train_id_workers = train_data['ASSIGNMENT:worker_id'].tolist()\n",
        "unique_id_workers = set(train_data['ASSIGNMENT:worker_id'])\n",
        "id_workers_mapping = {id_worker: i for i, id_worker in enumerate(unique_id_workers)}\n",
        "\n",
        "train_id_workers_indices = [id_workers_mapping[id_worker] for id_worker in train_id_workers]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HannRF7JeF2F"
      },
      "source": [
        "Define a ***class*** for the *train*, *val* and *test* dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKvD7P_PVKQX"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length, id_workers=None, load_id_workers=False):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.load_id_workers = load_id_workers\n",
        "        if load_id_workers:\n",
        "            self.id_workers = id_workers\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        tokens = self.tokenizer(text, max_length=self.max_length, padding='max_length', truncation=True)\n",
        "        item = {\n",
        "            'input_ids': torch.tensor(tokens['input_ids'], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(tokens['attention_mask'], dtype=torch.long),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "        if self.load_id_workers:\n",
        "            item['id_worker'] = torch.tensor(self.id_workers[idx], dtype=torch.long)\n",
        "        \n",
        "        return item\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4Vt9ymypu1e"
      },
      "source": [
        "Define a ***tokenizer*** for the text - we take a ready-made one from Hugging Face and prepare dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "is5dxFdpP9SC"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "#Choose the batch_size\n",
        "batch_size = 128\n",
        "max_length = 256\n",
        "\n",
        "train_dataset = TextDataset(train_data['INPUT:text'].tolist(), train_data['OUTPUT:result'].tolist(), tokenizer, max_length, train_id_workers_indices, load_id_workers=True)\n",
        "val_dataset = TextDataset(val_data['text'].tolist(), val_data['label'].tolist(), tokenizer, max_length)\n",
        "test_dataset = TextDataset(test_data['text'].tolist(), test_data['label'].tolist(), tokenizer, max_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The model with CoNAL layer"
      ],
      "metadata": {
        "id": "Wfgti39G-2_I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51Ct-ESzqHYl"
      },
      "source": [
        "The most interesting thing! We define our *model* and integrate our ***CoNAL*** layer into it to learn the model with non-aggregated data. \n",
        "\n",
        "*Common Noise Adaptation Layers* (CoNAL) introduces two types of confusions: worker-specific and global. Each is parameterized by a confusion matrix. The ratio of the two confusions is determined by the *common noise adaptation layer*. The *common noise adaptation layer* is a trainable function that takes the instance embedding and the worker ID as input and outputs a scalar value between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKCxvm1APnyA"
      },
      "outputs": [],
      "source": [
        "# LSTMClassifier\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "        self.conal = CoNAL(2, 1107) # 2 - the number of classes, 1107 - the number of unique workers\n",
        "\n",
        "    def forward(self, x, attention_mask=None, id_workers=None):\n",
        "        x = self.embedding(x)\n",
        "        output, _ = self.lstm(x)\n",
        "        out = self.fc(output[:, -1, :])\n",
        "        if id_workers is not None:\n",
        "            out = self.conal(output[:, -1, :], out, id_workers)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A7fXcJCqckC"
      },
      "source": [
        "The standart train pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9jQLNpUPnuy"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        id_workers = batch['id_worker'].to(device)\n",
        "\n",
        "        logits = model(input_ids, attention_mask, id_workers)\n",
        "        logits = model(input_ids, attention_mask)\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(logits, 1)\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = correct_predictions.double() / len(dataloader.dataset)\n",
        "\n",
        "    return epoch_loss, epoch_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZIqRTGdqkZW"
      },
      "source": [
        "We will also check out *loss* on the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaaOsuijPnsR"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(logits, 1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = correct_predictions.double() / len(dataloader.dataset)\n",
        "\n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLnGoZllquz4"
      },
      "source": [
        "Create an instance of the *model*, *optimizer*, and *loss-function*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1_hn0-MPnp-"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.vocab)\n",
        "embed_dim = 100\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "num_classes = 2\n",
        "\n",
        "model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, num_layers, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "torch.set_warn_always(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObtRClBTrI8K"
      },
      "source": [
        "Choose the ***number of the epoches*** and run the learning process!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnIDuw3TPnnw",
        "outputId": "a35eaa8f-9f6f-403a-e561-625bad5986ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "Train Loss: 0.6680 | Train Acc: 0.5984\n",
            "Val Loss: 0.6984 | Val Acc: 0.5376\n",
            "Model saved!\n",
            "Epoch 2/5\n",
            "Train Loss: 0.6178 | Train Acc: 0.6561\n",
            "Val Loss: 0.6074 | Val Acc: 0.6932\n",
            "Model saved!\n",
            "Epoch 3/5\n",
            "Train Loss: 0.5215 | Train Acc: 0.7523\n",
            "Val Loss: 0.5538 | Val Acc: 0.7342\n",
            "Model saved!\n",
            "Epoch 4/5\n",
            "Train Loss: 0.4611 | Train Acc: 0.7947\n",
            "Val Loss: 0.5203 | Val Acc: 0.7501\n",
            "Model saved!\n",
            "Epoch 5/5\n",
            "Train Loss: 0.4314 | Train Acc: 0.8120\n",
            "Val Loss: 0.5372 | Val Acc: 0.7405\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 5\n",
        "\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = eval_model(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "        print(\"Model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvolS3rQroT-"
      },
      "source": [
        "Let's calculate the ***final loss*** on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zDKffy8TPnjv",
        "outputId": "01626be1-af68-4784-a13f-de73f0533149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.5281 | Test Acc: 0.7441\n"
          ]
        }
      ],
      "source": [
        "best_model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, num_layers, num_classes)\n",
        "best_model.load_state_dict(torch.load('best_model.pt'))\n",
        "best_model = best_model.to(device)\n",
        "\n",
        "test_loss, test_acc = eval_model(best_model, test_loader, criterion, device)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOJIISEZsWBW"
      },
      "source": [
        "So, we trained our model on non-aggregated data using the ***CoNAL*** layer and got a good result."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The model with crowdlayer"
      ],
      "metadata": {
        "id": "mM6xxqZOARbC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fdep6xbubCJ"
      },
      "source": [
        "Now, let's see how the model will deal with ***crowdlayer***. \n",
        "\n",
        "It applies a worker-specific transformation of the logits. There are four types of transformations:\n",
        "\n",
        "- MW: Multiplication on the worker's confusion matrix.\n",
        "- VW: Element-wise multiplication with the worker's weight vector.\n",
        "- VB: Element-wise addition with the worker's bias vector.\n",
        "- VW + b: Combination of VW and VB: VW * logits + b."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PV-vkIlGvOwl"
      },
      "outputs": [],
      "source": [
        "# LSTMClassifier\n",
        "class LSTMClassifier_crowdlayer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "        self.crowdlaeyr = CrowdLayer(2, 1107, conn_type=\"mw\")\n",
        "\n",
        "    def forward(self, x, attention_mask=None, id_workers=None):\n",
        "        x = self.embedding(x)\n",
        "        output, _ = self.lstm(x)\n",
        "        out = self.fc(output[:, -1, :])\n",
        "        if id_workers is not None:\n",
        "            out = self.crowdlaeyr(out, id_workers)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCyhw4Sa0_8x"
      },
      "source": [
        "Create a *new* model with the same parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T3dgfLWUvPI4"
      },
      "outputs": [],
      "source": [
        "model_with_crowdlayer = LSTMClassifier_crowdlayer(vocab_size, embed_dim, hidden_dim, num_layers, num_classes)\n",
        "model_with_crowdlayer = model_with_crowdlayer.to(device)\n",
        "optimizer = optim.Adam(model_with_crowdlayer.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the same *number of epochs* so that both models are in equal conditions"
      ],
      "metadata": {
        "id": "OoWHj72nAnYq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "swiIHgqJvPGt",
        "outputId": "4c96f111-f07d-4866-9715-2f788027cced"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "Train Loss: 0.6686 | Train Acc: 0.5965\n",
            "Val Loss: 0.6506 | Val Acc: 0.6371\n",
            "Model saved!\n",
            "Epoch 2/5\n",
            "Train Loss: 0.6002 | Train Acc: 0.6808\n",
            "Val Loss: 0.6259 | Val Acc: 0.6586\n",
            "Model saved!\n",
            "Epoch 3/5\n",
            "Train Loss: 0.5213 | Train Acc: 0.7532\n",
            "Val Loss: 0.4736 | Val Acc: 0.7839\n",
            "Model saved!\n",
            "Epoch 4/5\n",
            "Train Loss: 0.4532 | Train Acc: 0.8009\n",
            "Val Loss: 0.4751 | Val Acc: 0.7851\n",
            "Model saved!\n",
            "Epoch 5/5\n",
            "Train Loss: 0.4230 | Train Acc: 0.8163\n",
            "Val Loss: 0.4560 | Val Acc: 0.7990\n",
            "Model saved!\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 5\n",
        "\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_model(model_with_crowdlayer, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = eval_model(model_with_crowdlayer, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model_with_crowdlayer.state_dict(), 'best_model_with_crowdlayer.pt')\n",
        "        print(\"Model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOEaxF7K1n-B"
      },
      "source": [
        "Let's check the ***final loss*** on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80hNwmiKvPED"
      },
      "outputs": [],
      "source": [
        "best_model_with_crowdlayer = LSTMClassifier_crowdlayer(vocab_size, embed_dim, hidden_dim, num_layers, num_classes)\n",
        "best_model_with_crowdlayer.load_state_dict(torch.load('best_model_with_crowdlayer.pt'))\n",
        "best_model_with_crowdlayer = best_model_with_crowdlayer.to(device)\n",
        "\n",
        "# Подсчет ошибки и точности на тестовом наборе данных\n",
        "test_loss, test_acc = eval_model(best_model_with_crowdlayer, test_loader, criterion, device)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "ysq8hnMaAUft"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_pODt0d2mLC"
      },
      "source": [
        "In this tutorial, we looked at how to train a model on non-aggregated data without losing accuracy on a test dataset using layers from the Crowd-Kit library"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}