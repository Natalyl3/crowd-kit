{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learning ML model on non-aggregated data"
      ],
      "metadata": {
        "id": "Eult4u-UWW-D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzIbbdhdia_s"
      },
      "source": [
        "Welcome to this tutorial!\n",
        "\n",
        "We all know that data is required to train the model, and the quality of this data greatly affects the final result. Training a model on non-aggregated data usually leads to a loss in model quality, so the Toloka team has implemented special layers for sublimating such data. In this notebook, we will solve this problem using the LSTM model and integrate the CoNAL and CrowdLayer layers from Crowd-Kit library!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries importing"
      ],
      "metadata": {
        "id": "chiZAWvU_bGX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgWLac4VjpTv"
      },
      "source": [
        "First of all, let's install and import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "neS64tr75OZs"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install crowd-kit\n",
        "%pip install sentence_transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from crowdkit.learning import CoNAL\n",
        "from crowdkit.learning import CrowdLayer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting with cuda"
      ],
      "metadata": {
        "id": "9fIbGnjO_8JJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T8-rLTjj-9F"
      },
      "source": [
        "It's important to boost our calculation speed so use the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VXUKMUa8tZz",
        "outputId": "0b54b4f9-8d14-489f-f6ec-04944412e470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "kkg5WEo9_3X5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP59rmCAkMET"
      },
      "source": [
        "We are going to use non-aggregated data. So, read the *train*, *val* and *test* data from csv-files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L8H63lBr5lJe"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"fixed_train_crowd_alpha047 (1).csv\")\n",
        "val_data = pd.read_csv(\"fixed_val_clean_alpha047.csv\")\n",
        "test_data = pd.read_csv(\"fixed_test.csv\")\n",
        "pd.options.mode.chained_assignment = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpIa7B22kkrY"
      },
      "source": [
        "Preparing our data: choose the necessary columns and change labels from '*pos*'-'*neg*' to '*1*'-'0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G6plSCtN5lMz"
      },
      "outputs": [],
      "source": [
        "train_data = train_data[['INPUT:text', 'OUTPUT:result', 'ASSIGNMENT:worker_id']]\n",
        "train_data['OUTPUT:result'] = train_data['OUTPUT:result'].replace('pos', int(1))\n",
        "train_data['OUTPUT:result'] = train_data['OUTPUT:result'].replace('neg', int(0))\n",
        "\n",
        "val_data = val_data[[\"text\", \"label\"]]\n",
        "val_data['label'] = val_data['label'].replace('pos', int(1))\n",
        "val_data['label'] = val_data['label'].replace('neg', int(0))\n",
        "\n",
        "test_data = test_data[['text', 'label']]\n",
        "test_data['label'] = test_data['label'].replace('pos', int(1))\n",
        "test_data['label'] = test_data['label'].replace('neg', int(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OKEjdb3l1KB"
      },
      "source": [
        "Since the **id_worker** field has the string type, we need to convert it to the int type. To do this, we will assign a unique string values to all unique int values in the field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "25U3pe_48XRs"
      },
      "outputs": [],
      "source": [
        "train_id_workers = train_data['ASSIGNMENT:worker_id']\n",
        "train_id_workers_indices, unique_ids = pd.factorize(train_id_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HannRF7JeF2F"
      },
      "source": [
        "Define a **class** for the *train*, *val* and *test* dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XKvD7P_PVKQX"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length, id_workers=None, load_id_workers=False):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.load_id_workers = load_id_workers\n",
        "        if load_id_workers:\n",
        "            self.id_workers = id_workers\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        tokens = self.tokenizer(text, max_length=self.max_length, padding='max_length', truncation=True)\n",
        "        item = {\n",
        "            'input_ids': torch.tensor(tokens['input_ids'], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(tokens['attention_mask'], dtype=torch.long),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "        if self.load_id_workers:\n",
        "            item['id_worker'] = torch.tensor(self.id_workers[idx], dtype=torch.long)\n",
        "        \n",
        "        return item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4Vt9ymypu1e"
      },
      "source": [
        "Define a **tokenizer** for the text - we take a ready-made one from Hugging Face and prepare dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "is5dxFdpP9SC"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Choose the batch_size\n",
        "batch_size = 128\n",
        "max_length = 256\n",
        "\n",
        "train_dataset = TextDataset(train_data['INPUT:text'], train_data['OUTPUT:result'], tokenizer, max_length, train_id_workers_indices, load_id_workers=True)\n",
        "val_dataset = TextDataset(val_data['text'], val_data['label'], tokenizer, max_length)\n",
        "test_dataset = TextDataset(test_data['text'], test_data['label'], tokenizer, max_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The model with CoNAL layer"
      ],
      "metadata": {
        "id": "Wfgti39G-2_I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51Ct-ESzqHYl"
      },
      "source": [
        "The most interesting thing! We define our *model* and integrate our **CoNAL** layer into it to learn the model with non-aggregated data. \n",
        "\n",
        "*Common Noise Adaptation Layers* (CoNAL) introduces two types of confusions: worker-specific and global. Each is parameterized by a confusion matrix. The ratio of the two confusions is determined by the *common noise adaptation layer*. The *common noise adaptation layer* is a trainable function that takes the instance embedding and the worker ID as input and outputs a scalar value between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YKCxvm1APnyA"
      },
      "outputs": [],
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "        self.conal = CoNAL(2, 1107)  # 2 - the number of classes, 1107 - the number of unique workers\n",
        "\n",
        "    def forward(self, x, attention_mask=None, id_workers=None):\n",
        "        x = self.embedding(x)\n",
        "        output, _ = self.lstm(x)\n",
        "        out = self.fc(output[:, -1, :])\n",
        "        if id_workers is not None:\n",
        "            out = self.conal(output[:, -1, :], out, id_workers)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A7fXcJCqckC"
      },
      "source": [
        "The standart train pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "q9jQLNpUPnuy"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        id_workers = batch['id_worker'].to(device)\n",
        "\n",
        "        logits = model(input_ids, attention_mask, id_workers)\n",
        "        logits = model(input_ids, attention_mask)\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(logits, 1)\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = correct_predictions.double() / len(dataloader.dataset)\n",
        "\n",
        "    return epoch_loss, epoch_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZIqRTGdqkZW"
      },
      "source": [
        "We will also check out *loss* on the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "NaaOsuijPnsR"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(logits, 1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = correct_predictions.double() / len(dataloader.dataset)\n",
        "\n",
        "    return epoch_loss, epoch_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLnGoZllquz4"
      },
      "source": [
        "Create an instance of the model, optimizer, and loss-function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1_hn0-MPnp-"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.vocab)\n",
        "embed_dim = 100\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "num_classes = 2\n",
        "\n",
        "model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, num_layers, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "torch.set_warn_always(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObtRClBTrI8K"
      },
      "source": [
        "Choose the *number of the epoches* and run the learning process!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnIDuw3TPnnw"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = eval_model(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "        print(\"Model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvolS3rQroT-"
      },
      "source": [
        "Let's calculate the **final loss** on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDKffy8TPnjv",
        "outputId": "aad92db0-2305-4266-a023-79bfd2542d7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.5155 | Test Acc: 0.7681\n"
          ]
        }
      ],
      "source": [
        "best_model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, num_layers, num_classes)\n",
        "best_model.load_state_dict(torch.load('best_model.pt'))\n",
        "best_model = best_model.to(device)\n",
        "\n",
        "test_loss, test_acc = eval_model(best_model, test_loader, criterion, device)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOJIISEZsWBW"
      },
      "source": [
        "So, we trained our model on non-aggregated data using the **CoNAL** layer and got a good result."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The model with crowdlayer"
      ],
      "metadata": {
        "id": "mM6xxqZOARbC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fdep6xbubCJ"
      },
      "source": [
        "Now, let's see how the model will deal with **crowdlayer**. \n",
        "\n",
        "It applies a worker-specific transformation of the logits. There are four types of transformations:\n",
        "\n",
        "- **MW**: Multiplication on the worker's confusion matrix.\n",
        "- **VW**: Element-wise multiplication with the worker's weight vector.\n",
        "- **VB**: Element-wise addition with the worker's bias vector.\n",
        "- **VW** + b: Combination of VW and VB: VW * logits + b."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "PV-vkIlGvOwl"
      },
      "outputs": [],
      "source": [
        "class LSTMClassifier_crowdlayer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "        self.crowdlaeyr = CrowdLayer(2, 1107, conn_type=\"mw\")\n",
        "\n",
        "    def forward(self, x, attention_mask=None, id_workers=None):\n",
        "        x = self.embedding(x)\n",
        "        output, _ = self.lstm(x)\n",
        "        out = self.fc(output[:, -1, :])\n",
        "        if id_workers is not None:\n",
        "            out = self.crowdlaeyr(out, id_workers)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCyhw4Sa0_8x"
      },
      "source": [
        "Create a *new* model with the same parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "T3dgfLWUvPI4"
      },
      "outputs": [],
      "source": [
        "model_with_crowdlayer = LSTMClassifier_crowdlayer(vocab_size, embed_dim, hidden_dim, num_layers, num_classes)\n",
        "model_with_crowdlayer = model_with_crowdlayer.to(device)\n",
        "optimizer = optim.Adam(model_with_crowdlayer.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the same *number of epochs* so that both models are in equal conditions"
      ],
      "metadata": {
        "id": "OoWHj72nAnYq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swiIHgqJvPGt"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_model(model_with_crowdlayer, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = eval_model(model_with_crowdlayer, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model_with_crowdlayer.state_dict(), 'best_model_with_crowdlayer.pt')\n",
        "        print(\"Model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOEaxF7K1n-B"
      },
      "source": [
        "Let's check the **final loss** on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "80hNwmiKvPED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88e5d490-f5bc-46e3-cf24-ae5687671baf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.4677 | Test Acc: 0.7817\n"
          ]
        }
      ],
      "source": [
        "best_model_with_crowdlayer = LSTMClassifier_crowdlayer(vocab_size, embed_dim, hidden_dim, num_layers, num_classes)\n",
        "best_model_with_crowdlayer.load_state_dict(torch.load('best_model_with_crowdlayer.pt'))\n",
        "best_model_with_crowdlayer = best_model_with_crowdlayer.to(device)\n",
        "\n",
        "test_loss, test_acc = eval_model(best_model_with_crowdlayer, test_loader, criterion, device)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary"
      ],
      "metadata": {
        "id": "ysq8hnMaAUft"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_pODt0d2mLC"
      },
      "source": [
        "In this tutorial, we looked at how to train a model on non-aggregated data without losing accuracy on a test dataset using layers from the Crowd-Kit library"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}